{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Percepto-Copy2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliashahbazi/VideoStimulus/blob/master/Percepto_Copy2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHkTLZTKIgbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "906a6c45-6c64-423a-c771-c7399882cc43"
      },
      "source": [
        "module_path = 'https://tfhub.dev/deepmind/biggan-deep-256/1'  # 256x256 BigGAN-deep\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from scipy.stats import truncnorm\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import io\n",
        "\n",
        "from pytorch_pretrained_biggan import BigGAN\n",
        "from torchvision.transforms import ToPILImage, Resize, Compose\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from io import BytesIO\n",
        "\n",
        "from numpy import genfromtxt\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import random\n",
        "from imagenet_clsidx_to_labels import idx_to_name_dict\n",
        "import torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-44254c2a86eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_biggan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBigGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToPILImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_biggan'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFMncyZpGmzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "aa8c39a1-38e2-4ffa-a7f0-6bcefbce0c20"
      },
      "source": [
        "module_path = 'https://tfhub.dev/deepmind/biggan-deep-256/1'  # 256x256 BigGAN-deep\n",
        "import io\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from scipy.stats import truncnorm\n",
        "import tensorflow as tf\n",
        "from pytorch_pretrained_biggan import BigGAN\n",
        "from torchvision.transforms import ToPILImage, Resize, Compose\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from io import BytesIO\n",
        "\n",
        "from numpy import genfromtxt\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import random\n",
        "from imagenet_clsidx_to_labels import idx_to_name_dict\n",
        "import torch\n",
        "\n",
        "def BG_dist(idx1,idx2,alpha,truncation):\n",
        "       def get_c_vector(idx1: int, idx2: int, alpha: float, batch_size: int=1):\n",
        "           c = torch.zeros(batch_size, 1000)\n",
        "           c[:, idx1] = 1-alpha\n",
        "           c[:, idx2] = alpha\n",
        "           return c\n",
        "\n",
        "       def get_z_vector(truncation: float, batch_size: int=1):\n",
        "           values = truncnorm.rvs(-2, 2, size=(batch_size, 128)).astype(np.float32)\n",
        "           values = torch.from_numpy(values)\n",
        "           return values * truncation\n",
        "\n",
        "       \n",
        "        \n",
        "       c = get_c_vector(idx1, idx2, alpha, batch_size=1)\n",
        "       z = get_z_vector(truncation, batch_size=1)\n",
        "\n",
        "       #print('C size =',c.size(),' Z size=',z.size())\n",
        "       #print(z) \n",
        "       print('Generating an image ...')\n",
        "       model = BigGAN.from_pretrained(f'biggan-deep-{512}')\n",
        "       with torch.no_grad():\n",
        "           out = model(z, c, truncation)\n",
        "\n",
        "       print('Image generated.')\n",
        "       final_transforms = Compose([\n",
        "           ToPILImage(),\n",
        "           Resize((1024, 1024))\n",
        "       ])\n",
        "\n",
        "       out = out.squeeze()\n",
        "       out = out/2 + 0.5\n",
        "       final = final_transforms(out)\n",
        "       return final,c,z\n",
        "       \n",
        " \n",
        "def manual_dist(c,z,truncation):\n",
        "       model = BigGAN.from_pretrained(f'biggan-deep-{512}')\n",
        "       with torch.no_grad():\n",
        "           out = model(z, c, truncation)\n",
        "\n",
        "       #print('Image generated.')\n",
        "       final_transforms = Compose([\n",
        "           ToPILImage(),\n",
        "           Resize((1024, 1024))\n",
        "       ])\n",
        "\n",
        "       out = out.squeeze()\n",
        "       out = out/2 + 0.5\n",
        "       final = final_transforms(out)\n",
        "       return final \n",
        "\n",
        "\n",
        "\n",
        "def img_show(imlist):\n",
        "    rand = random.randrange(0,len(imlist))\n",
        "    print(int(imlist[rand,5]) ,int(imlist[rand,6]),imlist[rand,7],imlist[rand,8])\n",
        "    [final,c,z] = BG_dist(int(imlist[rand,5]) ,int(imlist[rand,6]),imlist[rand,7],imlist[rand,8])\n",
        "    truncation = imlist[rand,8]\n",
        "    #plt.imshow(final)\n",
        "    #print(z[0,1])\n",
        "    return c,z,truncation\n",
        "    \n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        z[0,0]  = torch.FloatTensor([p.value])\n",
        "        z[0,10] = torch.FloatTensor([q.value])\n",
        "        z[0,100]= torch.FloatTensor([r.value])\n",
        "        nonzeroind = np.nonzero(torch.Tensor.numpy(c))[1]\n",
        "         \n",
        "        print(torch.Tensor.numpy(c[:,nonzeroind[0]]))\n",
        "        alpha = torch.Tensor.numpy(c[:,nonzeroind[0]])\n",
        "        idx1 = nonzeroind[0]\n",
        "        idx2 = nonzeroind[1]\n",
        "        \n",
        "        c[:,idx1]= torch.FloatTensor([alpha /2])\n",
        "        c[:,idx2] = torch.FloatTensor([alpha - (alpha/2)])\n",
        "        c[:,int(box1.value)]= torch.FloatTensor([(1-alpha)/2])\n",
        "        c[:,int(box2.value)]= torch.FloatTensor([(1-alpha)-(1-alpha)/2])\n",
        "        \n",
        "        \n",
        "        img = manual_dist(c,z,t)\n",
        "        f = BytesIO()\n",
        "        img.save(f,'png')\n",
        "        IPython.display.display(IPython.display.Image(data=f.getvalue(), width=200))\n",
        "        return f\n",
        "\n",
        "def view_image(i):\n",
        "    \n",
        "    #[c,z,truncation]=img_show(imlist)\n",
        "    z[0,0]=i\n",
        "    \n",
        "def box1(x):\n",
        "    return x\n",
        "    \n",
        "dtype = [('id', np.int32),('b_idx1',np.int32),('b_idx2',np.int32),('b_alpha',np.float64),('b_t',np.float64),('f_idx1',np.int32),('f_idx2',np.int32),('f_alpha',np.float64),('f_t',np.float64),('trans',np.float64),('dir',np.int32)]\n",
        "imlist = np.zeros(1, dtype=dtype)\n",
        "\n",
        "imlist = genfromtxt('human/psytest.csv',delimiter=\",\")\n",
        "\n",
        "[c,z,t]=img_show(imlist)\n",
        "#print(c)\n",
        "\n",
        "button = widgets.Button(description=\"Click Me!\")\n",
        "output = widgets.Output()\n",
        "\n",
        "p = widgets.FloatSlider()\n",
        "p.min = z[0,0]-5\n",
        "p.max = z[0,0]+5\n",
        "p.value = z[0,0]\n",
        "p.step = 0.1\n",
        "display(p)\n",
        "\n",
        "q = widgets.FloatSlider()\n",
        "q.min = z[0,10]-5\n",
        "q.max = z[0,10]+5\n",
        "q.value = z[0,10]\n",
        "q.step = 0.1\n",
        "display(q)\n",
        "\n",
        "r = widgets.FloatSlider()\n",
        "r.min = z[0,100]-5\n",
        "r.max = z[0,100]+5\n",
        "r.value = z[0,100]\n",
        "r.step = 0.1\n",
        "display(r)\n",
        "\n",
        "box1=widgets.Text(value='10')\n",
        "box2=widgets.Text(value='700')\n",
        "display(box1)\n",
        "display(box2)\n",
        "\n",
        "display(button, output)\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-28a44fc845c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtruncnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_biggan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBigGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToPILImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_biggan'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8m2HqTkGm0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VddZuKu1Gm0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL48lNrYGm0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PCot2IbGm1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amwQoN59JBU2",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}